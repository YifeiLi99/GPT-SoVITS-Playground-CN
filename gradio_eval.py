#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
GPT-SoVITS è¯„æµ‹ / å¯¹æ¯”ï¼ˆCSV æœ€å°æ ¸å¿ƒç‰ˆï¼Œæ•´åˆå…¥ä¸»æ§å°é£æ ¼ï¼‰
=================================================
å˜æ›´æ€»è§ˆï¼ˆä¸ä½ ç°æœ‰ gradio_eval.py ä¸€è‡´æ ¸å¿ƒã€å¯¹é½ gradio_asr é£æ ¼ï¼‰ï¼š
- [ADD] é¡¶éƒ¨å¯¼èˆªæŒ‰é’®ï¼ˆTTS/ç»“æœè¯„æµ‹/å¾®è°ƒ/æ•°æ®æ¸…æ´—/è¯­éŸ³æ¡¥æ¥/ä¸»æ§å°ï¼‰ï¼Œä¸ gradio_asr åŒæ­¥ï¼›
- [CHG] Gradio UI æ”¹ä¸º **3 ä¸ªè¾“å…¥æ¡†**ï¼ˆCSV_A è·¯å¾„ã€CSV_B è·¯å¾„ã€è¾“å‡ºç›®å½•ï¼‰+ **ä¸€é”®å¼€å§‹å¯¹æ¯”**ï¼›
- [ADD] ç»“æœå±•ç¤ºåŒºï¼š
    1) æ–‡æœ¬æ¡†å±•ç¤ºæœ¬æ¬¡æ±‡æ€»æŠ¥å‘Šï¼ˆå‡å€¼ CERã€å¯¹é½æ ·æœ¬æ•°ã€æ¨¡å¼ç­‰ï¼‰ï¼›
    2) è‡ªåŠ¨å†™å‡º detail.csv / summary.csvï¼ˆé»˜è®¤ç›¸å¯¹è·¯å¾„ ./eval_out/ï¼‰ï¼›é¡µé¢ä¸‹æ–¹å¯ç›´æ¥ä¸‹è½½ï¼›
- [CHG] å…¨æµç¨‹é»˜è®¤ä½¿ç”¨ **ç›¸å¯¹è·¯å¾„** è¾“å‡ºï¼Œç¬¦åˆä½ çš„é•¿æœŸè¦æ±‚ï¼›
- [CHG] CSV å†™å‡ºç»Ÿä¸€ä½¿ç”¨ `csv.QUOTE_ALL`ï¼Œé¿å…æœªåŠ å¼•å·çš„é£é™©ï¼›
- [KEEP] CLI åŠŸèƒ½ä¿æŒï¼Œå¯ `--ui 0` èµ°å‘½ä»¤è¡Œï¼›
- [TODO] åç»­å¯æŠŠ CONFIG æŒªå…¥ config.pyï¼ˆå·²é›†ä¸­åœ¨é¡¶éƒ¨ï¼‰ã€‚

ä¾èµ–ï¼šnumpy, pandas, tqdmï¼ˆå¯é€‰ rapidfuzzï¼‰; gradioï¼ˆv3/4 å…¼å®¹ï¼‰ã€‚
"""

from __future__ import annotations
import os
import csv
import argparse
import re
from dataclasses import dataclass
from typing import List, Optional, Tuple

import numpy as np
import pandas as pd

# =============================
# [ADD] ç»Ÿä¸€é…ç½®ï¼ˆåç»­å¯è¿ç§»åˆ° config.pyï¼‰
# =============================
CONFIG = {
    # ç•Œé¢ä¸ç«¯å£
    "LANGUAGE": "zh",
    "DEFAULT_PORT": 9881,
    "GRADIO_QUEUE": True,
    "GRADIO_SERVER_NAME": "0.0.0.0",
    # é¡¶éƒ¨ä¸»æ§å°åœ°å€ï¼ˆä¸ gradio_asr.py ä¸€è‡´ï¼‰
    "CONTROL_URL": "http://127.0.0.1:9900/",

    # åˆ—åå€™é€‰ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    "KEY_COLS": ["utt_id", "id", "key", "sid", "uid"],
    "WAV_COLS": ["wav", "wav_path", "path", "audio", "audio_path", "è¯­éŸ³æ–‡ä»¶", "æ–‡ä»¶è·¯å¾„"],
    "REF_COLS": ["text", "ref", "ref_text", "gt", "target", "å‚è€ƒæ–‡æœ¬", "åŸå§‹æ–‡æœ¬"],
    "ASR_A_COLS": ["asr_a", "hyp_a", "pred_a", "result_a", "transcript_a"],
    "ASR_B_COLS": ["asr_b", "hyp_b", "pred_b", "result_b", "transcript_b"],
    "ASR_COLS": ["asr", "hyp", "pred", "result", "transcript", "asrå¤„ç†æ–‡æœ¬"],

    # è¾“å‡ºæ–‡ä»¶åï¼ˆç›¸å¯¹è·¯å¾„ä¼˜å…ˆï¼‰
    "OUT_DIR_DEFAULT": "./eval_out",
    "OUT_DETAIL": "detail.csv",
    "OUT_SUMMARY": "summary.csv",
    "NORM_ENABLE": True,
    "NORM_STRIP_PUNCT": True,
    "NORM_STRIP_SPACES": True,
    "NORM_STRIP_INTERJ": True,
    "NORM_TO_SIMPLIFIED": True,  # è‹¥ç³»ç»Ÿæœªå®‰è£… openccï¼Œåˆ™è‡ªåŠ¨å›é€€ä¸º False
    "NORM_CACHE_DIR": "./eval_out/_norm_cache",

    "LONG_SENT_THRES": 12,
    "PUNC_FOR_PROSODY": "ï¼Œ,ã€‚.!ï¼?ï¼Ÿã€â€¦ï¼›;ï¼š:",
    "KEYWORDS_DEFAULT": ["å“‡","å•Š","å—¯","å˜¿å˜¿","å“ˆå“ˆ"],
    "TARGET_CHARS_PER_SEC": 6.0,  # ç›®æ ‡è¯­é€Ÿï¼ˆå­—ç¬¦/ç§’ï¼‰ï¼Œå¯æ ¹æ®è¯­æ–™è°ƒæ•´

}

# =============================
# [ADD] è½»é‡ Levenshteinï¼ˆå¯é€‰ rapidfuzz åŠ é€Ÿï¼‰
# =============================
# =============================
# [ADD] è¯„æµ‹æŒ‡æ ‡å®ç°ï¼ˆCER/WER/å…³é”®è¯/åå­—/éŸµå¾‹/è¯­é€Ÿï¼‰
# =============================

# [ADD] åˆ†è¯/æ ‡è®°ï¼šä¸­æ–‡æ— ç©ºæ ¼æ—¶æŒ‰â€œä¸­è‹±æ–‡æ··åˆâ€ç­–ç•¥åˆ‡åˆ†ï¼›è‹±æ–‡/å¸¦ç©ºæ ¼ç”¨ç©ºæ ¼åˆ‡åˆ†
_word_re = re.compile(r"[A-Za-z0-9]+|[\u4e00-\u9fff]|[^\s]")

def tokenize_for_wer(s: str):
    s = (s or "").strip()
    if not s:
        return []
    # å¦‚æœæœ‰ç©ºæ ¼ï¼Œä¼˜å…ˆæŒ‰ç©ºæ ¼ï¼ˆè‹±æ–‡/å¸¦ç©ºæ ¼è¯­æ–™ï¼‰
    if " " in s.strip():
        return s.split()
    # å¦åˆ™æŒ‰â€œè‹±æ–‡ä¸²/å•ä¸ªä¸­æ–‡æ±‰å­—/å…¶ä»–å•å­—ç¬¦â€æ··åˆåˆ‡åˆ†
    return _word_re.findall(s)

def wer_word(ref: str, hyp: str) -> float:
    R = tokenize_for_wer(ref)
    H = tokenize_for_wer(hyp)
    if len(R) == 0:
        return 0.0 if len(H) == 0 else 1.0
    # ç»å…¸ Levenshtein on tokens
    n, m = len(R), len(H)
    dp = [[0]*(m+1) for _ in range(n+1)]
    for i in range(n+1):
        dp[i][0] = i
    for j in range(m+1):
        dp[0][j] = j
    for i in range(1, n+1):
        for j in range(1, m+1):
            cost = 0 if R[i-1] == H[j-1] else 1
            dp[i][j] = min(dp[i-1][j] + 1,     # deletion
                           dp[i][j-1] + 1,     # insertion
                           dp[i-1][j-1] + cost)  # substitution
    return dp[n][m] / max(1, n)

# [ADD] ç¼–è¾‘è·ç¦»æ“ä½œç»Ÿè®¡ï¼ˆç”¨äºâ€œåå­—ç‡â€=åˆ é™¤æ¯”ä¾‹ï¼‰
def edit_ops(ref: str, hyp: str):
    R = list(ref or "")
    H = list(hyp or "")
    n, m = len(R), len(H)
    dp = [[0]*(m+1) for _ in range(n+1)]
    bt = [[None]*(m+1) for _ in range(n+1)]
    for i in range(n+1):
        dp[i][0] = i
        if i>0: bt[i][0] = ('D', i-1, 0)
    for j in range(m+1):
        dp[0][j] = j
        if j>0: bt[0][j] = ('I', 0, j-1)
    for i in range(1, n+1):
        for j in range(1, m+1):
            if R[i-1] == H[j-1]:
                dp[i][j] = dp[i-1][j-1]
                bt[i][j] = ('M', i-1, j-1)
            else:
                # choose best among D, I, S
                cand = [
                    (dp[i-1][j] + 1, 'D', i-1, j),    # deletion
                    (dp[i][j-1] + 1, 'I', i, j-1),    # insertion
                    (dp[i-1][j-1] + 1, 'S', i-1, j-1) # substitution
                ]
                best = min(cand, key=lambda x:x[0])
                dp[i][j] = best[0]
                bt[i][j] = (best[1], best[2], best[3])
    # backtrace
    i, j = n, m
    dels = ins = subs = match = 0
    while i>0 or j>0:
        op, pi, pj = bt[i][j] if bt[i][j] else ('M', i-1, j-1)
        if op == 'D': dels += 1
        elif op == 'I': ins += 1
        elif op == 'S': subs += 1
        else: match += 1
        i, j = pi, pj
    return {'del':dels, 'ins':ins, 'sub':subs, 'match':match, 'dist':dp[n][m], 'n_ref':n, 'n_hyp':m}

# [ADD] åå­—ç‡ï¼ˆé•¿å¥ï¼‰ï¼šåˆ é™¤æ¯”ä¾‹ï¼Œä»…åœ¨ ref_len >= é˜ˆå€¼ æ—¶è®¡å…¥
def swallow_rate(ref: str, hyp: str, long_thres: int = 20) -> Optional[float]:
    ref = ref or ""
    hyp = hyp or ""
    if len(ref) < long_thres:
        return None
    ops = edit_ops(ref, hyp)
    return ops['del'] / max(1, ops['n_ref'])

# [ADD] åŸºäºæ ‡ç‚¹çš„â€œéŸµå¾‹æŠ–åŠ¨æ–¹å·®â€ï¼ˆä»£ç†æŒ‡æ ‡ï¼‰ï¼š
#      ä»¥æ ‡ç‚¹åˆ†æ®µï¼Œè®¡ç®—â€œæ¯æ®µé•¿åº¦å·®ï¼ˆhyp-refï¼‰â€çš„æ–¹å·®/å‡æ–¹è¯¯å·®
PUNC_DEFAULT = "ï¼Œ,ã€‚.!ï¼?ï¼Ÿã€â€¦ï¼›;ï¼š:"

def _segments_by_punc(s: str, punc: str) -> list[int]:
    buf = 0
    arr = []
    for ch in s or "":
        if ch in punc:
            arr.append(buf)
            buf = 0
        else:
            buf += 1
    arr.append(buf)
    return arr

def prosody_jitter_var(ref: str, hyp: str, punc: str = PUNC_DEFAULT) -> float:
    a = _segments_by_punc(ref or "", punc)
    b = _segments_by_punc(hyp or "", punc)
    # å¯¹é½è¾ƒçŸ­é•¿åº¦
    L = min(len(a), len(b)) if a and b else 0
    if L == 0:
        return 0.0
    diffs = [(b[i]-a[i]) for i in range(L)]
    # å½’ä¸€åŒ–åˆ° ref æ®µé•¿ï¼Œé¿å…é•¿åº¦è§„æ¨¡å½±å“
    norm = [ (diff / (a[i] if a[i]>0 else 1)) for i, diff in enumerate(diffs) ]
    # è¿”å›å‡æ–¹ï¼ˆæ–¹å·®ä»£ç†ï¼‰
    return float(np.mean([x*x for x in norm]))

# [ADD] å…³é”®è¯ä¸€è‡´æ€§ï¼šè®¡ç®— refâ†’hyp çš„â€œå¬å›ç‡â€ï¼Œä»¥åŠ hyp ä¸­â€œé¢å¤–å…³é”®è¯â€æ¯”ç‡
def keyword_metrics(ref: str, hyp: str, keywords: List[str]) -> Tuple[Optional[float], Optional[float]]:
    kws = [k for k in (keywords or []) if k]
    if not kws:
        return None, None
    ref_hits = sum(1 for k in kws if k in (ref or ""))
    hyp_hits = sum(1 for k in kws if k in (hyp or ""))
    recall = (hyp_hits / ref_hits) if ref_hits>0 else (None)
    # é¢å¤–å…³é”®è¯ï¼šhyp ä¸­å‡ºç°ä½† ref ä¸­æœªå‡ºç°çš„å æ¯”
    extra = sum(1 for k in kws if (k in (hyp or "")) and (k not in (ref or "")))
    extra_ratio = (extra / len(kws)) if kws else None
    return recall, extra_ratio

# [ADD] è¯­é€Ÿè¯¯å·®ï¼šéœ€è¦ wav æ—¶é•¿ï¼›è‹¥æ‹¿ä¸åˆ° wav åˆ™è¿”å› None
def _wav_duration_seconds(path: str) -> Optional[float]:
    try:
        import soundfile as sf
        import numpy as np  # noqa: F401
        with sf.SoundFile(path) as f:
            frames = len(f)
            sr = f.samplerate
        return frames / float(sr)
    except Exception:
        try:
            import wave
            with wave.open(path, 'rb') as wf:
                frames = wf.getnframes()
                sr = wf.getframerate()
            return frames / float(sr)
        except Exception:
            return None

def speed_error(ref: str, wav_path: Optional[str], target_chars_per_sec: Optional[float]) -> Optional[float]:
    if not wav_path or not target_chars_per_sec or target_chars_per_sec <= 0:
        return None
    dur = _wav_duration_seconds(wav_path)
    if not dur or dur <= 0:
        return None
    cps = (len(ref or "")) / dur
    return abs(cps - target_chars_per_sec) / target_chars_per_sec  # ç»å¯¹è¯¯å·®æ¯”


HAVE_RAPIDFUZZ = False
try:
    from rapidfuzz.distance import Levenshtein  # type: ignore
    HAVE_RAPIDFUZZ = True
except Exception:
    pass


def _levenshtein_py(a: str, b: str) -> int:
    n, m = len(a), len(b)
    if n == 0:
        return m
    if m == 0:
        return n
    dp = list(range(m + 1))
    for i in range(1, n + 1):
        prev = dp[0]
        dp[0] = i
        for j in range(1, m + 1):
            tmp = dp[j]
            cost = 0 if a[i - 1] == b[j - 1] else 1
            dp[j] = min(dp[j] + 1, dp[j - 1] + 1, prev + cost)
            prev = tmp
    return dp[m]


def cer_char(ref: str, hyp: str) -> float:
    ref = ref or ""
    hyp = hyp or ""
    if ref == "":
        return 0.0 if hyp == "" else 1.0
    if HAVE_RAPIDFUZZ:
        dist = Levenshtein.distance(ref, hyp)
    else:
        dist = _levenshtein_py(ref, hyp)
    return dist / max(1, len(ref))

# =============================
# [ADD] CSV è§£æ & åˆ—è§„èŒƒåŒ–
# =============================

# =============================
# [ADD] æ–‡æœ¬æ ‡å‡†åŒ–ï¼ˆç®€ç¹/æ ‡ç‚¹/ç©ºç™½/è¯­æ°”å­—/NFKCï¼‰
# =============================
def _try_opencc():
    try:
        from opencc import OpenCC  # type: ignore
        return OpenCC('t2s')
    except Exception:
        return None

_OPENCC = _try_opencc()
if _OPENCC is None:
    CONFIG["NORM_TO_SIMPLIFIED"] = False  # å›é€€ï¼šæœªå®‰è£… opencc åˆ™ä¸åšç®€ç¹è½¬æ¢

_INTERJ_SET = set(list("å“‡å•Šå“¦å–”å™¢æ¬¸è¯¶å‘ƒå—¯å””å‘€å˜¿å“ˆå‘µå˜»å“¼å‘œ"))  # å¯å†æ‰©å……

import unicodedata

def _nfkc(s: str) -> str:
    try:
        return unicodedata.normalize("NFKC", s or "")
    except Exception:
        return s or ""

def _to_simplified(s: str) -> str:
    if CONFIG.get("NORM_TO_SIMPLIFIED", False) and _OPENCC is not None:
        try:
            return _OPENCC.convert(s)
        except Exception:
            return s
    return s

def _strip_punct_and_spaces(s: str) -> str:
    # å»æ‰€æœ‰ Unicode æ ‡ç‚¹ä¸ç©ºç™½
    return "".join(ch for ch in s if not unicodedata.category(ch).startswith("P") and not ch.isspace())

def _strip_interjections(s: str) -> str:
    # ç®€å•é€å­—å‰”é™¤å¸¸è§è¯­æ°”å­—ï¼ˆå·¥ç¨‹å–èˆï¼šé¿å…æŠŠâ€œå†…å®¹è¯â€è¯¯åˆ ï¼‰
    return "".join(ch for ch in s if ch not in _INTERJ_SET)

def normalize_text(s: str) -> str:
    if s is None:
        s = ""
    s = str(s)
    s = _nfkc(s)
    s = _to_simplified(s)
    if CONFIG.get("NORM_STRIP_PUNCT", True) or CONFIG.get("NORM_STRIP_SPACES", True):
        s = _strip_punct_and_spaces(s)
    if CONFIG.get("NORM_STRIP_INTERJ", True):
        s = _strip_interjections(s)
    return s

def normalize_dataframe(std: StdFrame, tag: str) -> StdFrame:
    """
    è¿”å›æ–°çš„ StdFrameï¼Œdf æ–‡æœ¬åˆ—å·²æ ‡å‡†åŒ–ï¼›å¹¶æŠŠæ ‡å‡†åŒ–åçš„ CSV å­˜åˆ° NORM_CACHE_DIR ä¸‹ã€‚
    ä»…å¤„ç†ï¼šref/asr/asr_a/asr_bï¼ˆå…¶ä½™åˆ—ä¿æŒä¸åŠ¨ï¼‰ã€‚
    """
    import os, pandas as pd, csv
    df = std.df.copy()
    cols = [c for c in [std.ref, std.asr, std.asr_a, std.asr_b] if c is not None]
    if cols and CONFIG.get("NORM_ENABLE", True):
        for c in cols:
            df[c] = df[c].astype(str).map(normalize_text)
    # å†™å…¥ç¼“å­˜
    out_dir = CONFIG.get("NORM_CACHE_DIR", "./eval_out/_norm_cache")
    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, f"{tag}_normalized.csv")
    df.to_csv(out_path, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_ALL)
    # è¿”å›æ–°çš„ StdFrame
    std2 = StdFrame(df=df, key=std.key, wav=std.wav, ref=std.ref, asr=std.asr, asr_a=std.asr_a, asr_b=std.asr_b)
    return std2

LOWER = lambda s: (s or "").strip().lower()


def _find_col(df: pd.DataFrame, cands: List[str]) -> Optional[str]:
    lower_map = {LOWER(c): c for c in df.columns}
    for k in cands:
        if LOWER(k) in lower_map:
            return lower_map[LOWER(k)]
    return None


@dataclass
class StdFrame:
    df: pd.DataFrame
    key: Optional[str]
    wav: Optional[str]
    ref: Optional[str]
    asr: Optional[str]
    asr_a: Optional[str]
    asr_b: Optional[str]


def standardize_csv(path: str) -> StdFrame:
    # å®¹é”™è¯»å–ç¼–ç 
    try:
        df = pd.read_csv(path, encoding="utf-8")
    except Exception:
        df = pd.read_csv(path, encoding="utf-8-sig")

    key = _find_col(df, CONFIG["KEY_COLS"]) or None
    wav = _find_col(df, CONFIG["WAV_COLS"]) or None
    ref = _find_col(df, CONFIG["REF_COLS"]) or None
    asr = _find_col(df, CONFIG["ASR_COLS"]) or None
    asr_a = _find_col(df, CONFIG["ASR_A_COLS"]) or None
    asr_b = _find_col(df, CONFIG["ASR_B_COLS"]) or None
    return StdFrame(df=df, key=key, wav=wav, ref=ref, asr=asr, asr_a=asr_a, asr_b=asr_b)


# =============================
# [ADD] å¯¹é½ç­–ç•¥ï¼šç»å¯¹ wav è·¯å¾„ > key > æ–‡ä»¶å > è¡Œå·
# =============================

def _norm_path(p: str) -> str:
    try:
        return os.path.normpath(os.path.abspath(p))
    except Exception:
        return (p or "").strip()


def build_join_key(std: StdFrame) -> pd.Series:
    df = std.df
    key_series = None

    if std.wav is not None:
        key_series = df[std.wav].astype(str).map(_norm_path)
    if key_series is None and std.key is not None:
        key_series = df[std.key].astype(str)
    if key_series is None and std.wav is not None:
        key_series = df[std.wav].astype(str).map(lambda x: os.path.basename(str(x)))
    if key_series is None:
        key_series = pd.Series([f"row_{i}" for i in range(len(df))], index=df.index)
    return key_series

# =============================
# [ADD] ä¸‰ç§å¯¹æ¯”æ¨¡å¼
# =============================
@dataclass
class CompareResult:
    detail: pd.DataFrame
    summary: pd.DataFrame


def compare_single(std: StdFrame) -> CompareResult:
    df = std.df.copy()
    join_key = build_join_key(std)

    # ä¼˜å…ˆï¼šASR_A vs ASR_B
    if std.asr_a is not None and std.asr_b is not None:
        A = df[std.asr_a].astype(str)
        B = df[std.asr_b].astype(str)
        cer_ab = [cer_char(a, b) for a, b in zip(A, B)]
        detail = pd.DataFrame({
            "key": join_key,
            "asr_A": A,
            "asr_B": B,
            "CER_A_vs_B": np.round(cer_ab, 4),
        })
        summary = pd.DataFrame([{
            "mode": "single_csv_asrA_vs_asrB",
            "n": len(detail),
            "CER_A_vs_B_mean": float(np.mean(detail["CER_A_vs_B"])) if len(detail) else np.nan,
        }])
        return CompareResult(detail, summary)

    # å…¶æ¬¡ï¼šASR vs REF
    if std.asr is not None and std.ref is not None:
        H = df[std.asr].astype(str)
        R = df[std.ref].astype(str)
        cer_hr = [cer_char(r, h) for r, h in zip(R, H)]
        detail = pd.DataFrame({
            "key": join_key,
            "ref": R,
            "asr": H,
            "CER_ASR_vs_REF": np.round(cer_hr, 4),
        })
        summary = pd.DataFrame([{
            "mode": "single_csv_asr_vs_ref",
            "n": len(detail),
            "CER_ASR_vs_REF_mean": float(np.mean(detail["CER_ASR_vs_REF"])) if len(detail) else np.nan,
        }])
        return CompareResult(detail, summary)

    raise ValueError("å•CSVå¯¹æ¯”å¤±è´¥ï¼šæœªæ£€æµ‹åˆ° (asr_A, asr_B) æˆ– (asr, ref) åˆ—")


def compare_two(stdA: StdFrame, stdB: StdFrame) -> CompareResult:
    # [KEEP] åŸæœ‰ A/B æ–‡æœ¬ç›´æ¥ CER å¯¹æ¯”ï¼ˆéå¸¸ç”¨ï¼‰

    dfA = stdA.df.copy()
    dfB = stdB.df.copy()
    keyA = build_join_key(stdA)
    keyB = build_join_key(stdB)

    def pick_text(std: StdFrame) -> Tuple[Optional[str], str]:
        if std.asr is not None:
            return std.asr, "asr"
        if std.asr_a is not None:
            return std.asr_a, "asr_a"
        if std.asr_b is not None:
            return std.asr_b, "asr_b"
        if std.ref is not None:
            return std.ref, "ref_as_fallback"
        return None, ""

    colA, tagA = pick_text(stdA)
    colB, tagB = pick_text(stdB)
    if colA is None or colB is None:
        raise ValueError("åŒCSVå¯¹æ¯”å¤±è´¥ï¼šè‡³å°‘ä¸€ä¾§æœªæ‰¾åˆ°å¯ç”¨æ–‡æœ¬åˆ—(asr/asr_a/asr_b/ref)")

    A = pd.DataFrame({"key": keyA, "text_A": dfA[colA].astype(str)})
    B = pd.DataFrame({"key": keyB, "text_B": dfB[colB].astype(str)})

    merged = A.merge(B, on="key", how="inner")
    if merged.empty:
        raise ValueError("åŒCSVå¯¹æ¯”å¤±è´¥ï¼šä¸¤ä¾§æŒ‰ key æ— å¯å¯¹é½æ ·æœ¬ï¼ˆæ£€æŸ¥ wav è·¯å¾„/utt_idï¼‰")

    cer_ab = [cer_char(a, b) for a, b in zip(merged["text_A"], merged["text_B"])]
    detail = pd.DataFrame({
        "key": merged["key"],
        "text_A": merged["text_A"],
        "text_B": merged["text_B"],
        "CER_A_vs_B": np.round(cer_ab, 4),
        "A_source": tagA,
        "B_source": tagB,
    })

    summary = pd.DataFrame([{
        "mode": "two_csv_asr_vs_asr",
        "n_join": int(len(detail)),
        "CER_A_vs_B_mean": float(np.mean(detail["CER_A_vs_B"])) if len(detail) else np.nan,
        "A_source": tagA,
        "B_source": tagB,
    }])
    return CompareResult(detail, summary)


# [ADD] æ‰©å±•ï¼šåŒ CSV çš„å®Œæ•´æŒ‡æ ‡ï¼ˆä»¥ A.ref ä¼˜å…ˆä½œä¸ºåŸºå‡†ï¼›è‹¥ B.ref å­˜åœ¨ä¸”ä¸€è‡´åˆ™æ— å½±å“ï¼‰
def compare_two_full(stdA: StdFrame, stdB: StdFrame, long_thres: int, punc: str, keywords: List[str], target_cps: Optional[float]) -> CompareResult:
    dfA = stdA.df.copy()
    dfB = stdB.df.copy()
    keyA = build_join_key(stdA)
    keyB = build_join_key(stdB)

    # é€‰æ‹©æ–‡æœ¬åˆ—
    if stdA.asr is None or stdB.asr is None:
        raise ValueError("åŒCSVå®Œæ•´æŒ‡æ ‡ï¼šä¸¤ä¾§éƒ½éœ€å­˜åœ¨ ASR åˆ—ï¼ˆå¦‚ 'asr' æˆ– 'asrå¤„ç†æ–‡æœ¬'ï¼‰")

    if stdA.ref is None and stdB.ref is None:
        raise ValueError("åŒCSVå®Œæ•´æŒ‡æ ‡ï¼šè‡³å°‘ä¸€ä¾§éœ€æä¾› REF åˆ—ï¼ˆå¦‚ 'text'/'åŸå§‹æ–‡æœ¬'ï¼‰")

    # åˆå¹¶å¯¹é½
    A = pd.DataFrame({"key": keyA, "ref_A": dfA[stdA.ref] if stdA.ref else None, "asr_A": dfA[stdA.asr].astype(str), "wav_A": dfA[stdA.wav] if stdA.wav else None})
    B = pd.DataFrame({"key": keyB, "ref_B": dfB[stdB.ref] if stdB.ref else None, "asr_B": dfB[stdB.asr].astype(str), "wav_B": dfB[stdB.wav] if stdB.wav else None})
    merged = A.merge(B, on="key", how="inner")

    if merged.empty:
        raise ValueError("åŒCSVå®Œæ•´æŒ‡æ ‡ï¼šæŒ‰ key æ— å¯¹é½æ ·æœ¬ï¼ˆæ£€æŸ¥ 'æ–‡ä»¶åœ°å€/utt_id/æ–‡ä»¶å' ç­‰åˆ—ï¼‰")

    # åŸºå‡† REFï¼šä¼˜å…ˆ A çš„ refï¼›è‹¥ç¼ºå¤±åˆ™ç”¨ B çš„ ref
    def choose_ref(row):
        return (row["ref_A"] if pd.notna(row.get("ref_A", None)) else row.get("ref_B", "")) or ""

    rows = []
    for _, r in merged.iterrows():
        ref = str(choose_ref(r))
        asrA = str(r["asr_A"])
        asrB = str(r["asr_B"])
        wavA = str(r["wav_A"]) if pd.notna(r.get("wav_A")) else None
        wavB = str(r["wav_B"]) if pd.notna(r.get("wav_B")) else None

        cer_A = cer_char(ref, asrA)
        cer_B = cer_char(ref, asrB)
        wer_A = wer_word(ref, asrA)
        wer_B = wer_word(ref, asrB)

        sr_A = swallow_rate(ref, asrA, long_thres)
        sr_B = swallow_rate(ref, asrB, long_thres)

        pjv_A = prosody_jitter_var(ref, asrA, punc)
        pjv_B = prosody_jitter_var(ref, asrB, punc)

        kw_rec_A, kw_extra_A = keyword_metrics(ref, asrA, keywords)
        kw_rec_B, kw_extra_B = keyword_metrics(ref, asrB, keywords)

        se_A = speed_error(ref, wavA, target_cps)
        se_B = speed_error(ref, wavB, target_cps)

        rows.append({
            "key": r["key"],
            "ref": ref,
            "asr_A": asrA,
            "asr_B": asrB,
            "CER_A": round(cer_A, 4),
            "CER_B": round(cer_B, 4),
            "WER_A": round(wer_A, 4),
            "WER_B": round(wer_B, 4),
            "Swallow_A": (round(sr_A,4) if sr_A is not None else None),
            "Swallow_B": (round(sr_B,4) if sr_B is not None else None),
            "ProsodyVar_A": round(pjv_A, 6),
            "ProsodyVar_B": round(pjv_B, 6),
            "KW_Recall_A": (round(kw_rec_A,4) if kw_rec_A is not None else None),
            "KW_Recall_B": (round(kw_rec_B,4) if kw_rec_B is not None else None),
            "KW_Extra_A": (round(kw_extra_A,4) if kw_extra_A is not None else None),
            "KW_Extra_B": (round(kw_extra_B,4) if kw_extra_B is not None else None),
            "SpeedErr_A": (round(se_A,4) if se_A is not None else None),
            "SpeedErr_B": (round(se_B,4) if se_B is not None else None),
        })

    detail = pd.DataFrame(rows)
    # åªåœ¨éç©ºæ—¶è®¡ç®—å‡å€¼
    def _mean(series):
        try:
            return float(pd.to_numeric(series, errors="coerce").dropna().mean())
        except Exception:
            return float("nan")

    summary = pd.DataFrame([{
        "mode": "two_csv_metrics_vs_ref",
        "n_join": int(len(detail)),
        "CER_mean_A": _mean(detail["CER_A"]),
        "CER_mean_B": _mean(detail["CER_B"]),
        "WER_mean_A": _mean(detail["WER_A"]),
        "WER_mean_B": _mean(detail["WER_B"]),
        "Swallow_mean_A": _mean(detail["Swallow_A"]),
        "Swallow_mean_B": _mean(detail["Swallow_B"]),
        "ProsodyVar_mean_A": _mean(detail["ProsodyVar_A"]),
        "ProsodyVar_mean_B": _mean(detail["ProsodyVar_B"]),
        "KW_Recall_mean_A": _mean(detail["KW_Recall_A"]),
        "KW_Recall_mean_B": _mean(detail["KW_Recall_B"]),
        "KW_Extra_mean_A": _mean(detail["KW_Extra_A"]),
        "KW_Extra_mean_B": _mean(detail["KW_Extra_B"]),
        "SpeedErr_mean_A": _mean(detail["SpeedErr_A"]),
        "SpeedErr_mean_B": _mean(detail["SpeedErr_B"]),
        # A/B å·®å€¼ï¼ˆB-Aï¼Œæ­£æ•°= B æ›´å¤§ï¼Œè´Ÿæ•°= B æ›´å°/æ›´å¥½ï¼‰
        "CER_delta_B_minus_A": _mean(detail["CER_B"]) - _mean(detail["CER_A"]),
        "WER_delta_B_minus_A": _mean(detail["WER_B"]) - _mean(detail["WER_A"]),
        "Swallow_delta_B_minus_A": _mean(detail["Swallow_B"]) - _mean(detail["Swallow_A"]),
        "ProsodyVar_delta_B_minus_A": _mean(detail["ProsodyVar_B"]) - _mean(detail["ProsodyVar_A"]),
        "KW_Recall_delta_B_minus_A": _mean(detail["KW_Recall_B"]) - _mean(detail["KW_Recall_A"]),
        "KW_Extra_delta_B_minus_A": _mean(detail["KW_Extra_B"]) - _mean(detail["KW_Extra_A"]),
        "SpeedErr_delta_B_minus_A": _mean(detail["SpeedErr_B"]) - _mean(detail["SpeedErr_A"]),
    }])
    return CompareResult(detail, summary)

# =============================
# [ADD] CLI / UI å…±ç”¨å…¥å£
# =============================

def _write_outputs(res: CompareResult, out_dir: str) -> Tuple[str, str]:
    os.makedirs(out_dir, exist_ok=True)
    detail_path = os.path.join(out_dir, CONFIG["OUT_DETAIL"])  # ç›¸å¯¹è·¯å¾„è¾“å‡º
    summary_path = os.path.join(out_dir, CONFIG["OUT_SUMMARY"])  # ç›¸å¯¹è·¯å¾„è¾“å‡º
    # ç»Ÿä¸€åŠ å¼•å·å†™å‡º
    res.detail.to_csv(detail_path, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_ALL)
    res.summary.to_csv(summary_path, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_ALL)
    return detail_path, summary_path


def _summary_text(res: CompareResult) -> str:
    # ç´§å‡‘å¯è¯»çš„æŠ¥å‘Šæ‘˜è¦ï¼ˆæ”¾åœ¨ UI æ–‡æœ¬æ¡†é‡Œï¼‰
    s = []
    mode = str(res.summary.iloc[0].get("mode", "-") if len(res.summary) else "-")
    s.append(f"[MODE] {mode}")
    for col in res.summary.columns:
        if col == "mode":
            continue
        val = res.summary.iloc[0].get(col)
        s.append(f"{col}: {val}")
    return "\n".join(s)


class _Args:  # ç®€æ˜“å‘½åç©ºé—´
    def __init__(self, csv_a: Optional[str], csv_b: Optional[str], out_dir: Optional[str]):
        self.csv_a = csv_a
        self.csv_b = csv_b
        self.out_dir = out_dir or CONFIG["OUT_DIR_DEFAULT"]


def run_cli(args) -> Tuple[str, str, str]:
    out_dir = args.out_dir or CONFIG["OUT_DIR_DEFAULT"]
    csvA = (args.csv_a or "").strip()
    csvB = (args.csv_b or "").strip()

    if (not csvA) and (not csvB):
        raise ValueError("è¯·è‡³å°‘æä¾› --csv_a æˆ–åŒæ—¶æä¾› --csv_a --csv_b")
    if csvA and (not os.path.isfile(csvA)):
        raise FileNotFoundError(f"csv_a ä¸å­˜åœ¨ï¼š{csvA}")
    if csvB and (not os.path.isfile(csvB)):
        raise FileNotFoundError(f"csv_b ä¸å­˜åœ¨ï¼š{csvB}")

    if csvA and csvB:
        stdA = standardize_csv(csvA)
        stdB = standardize_csv(csvB)
        # [ADD] æ¯”è¾ƒå‰åšæ ‡å‡†åŒ–å¹¶å†™ç¼“å­˜ CSV
        stdA = normalize_dataframe(stdA, 'A')
        stdB = normalize_dataframe(stdB, 'B')
        try:
            res = compare_two_full(stdA, stdB, CONFIG['LONG_SENT_THRES'], CONFIG['PUNC_FOR_PROSODY'], CONFIG['KEYWORDS_DEFAULT'], CONFIG['TARGET_CHARS_PER_SEC'])
        except Exception as _:
            # å›é€€åˆ°åŸæœ‰ç®€å• CER å¯¹æ¯”
            res = compare_two(stdA, stdB)
    else:
        std = standardize_csv(csvA or csvB)
        std = normalize_dataframe(std, 'SINGLE')
        res = compare_single(std)

    detail_path, summary_path = _write_outputs(res, out_dir)
    report = _summary_text(res)

    # æ§åˆ¶å°è¾“å‡º
    print("\n===== SUMMARY =====")
    print(report)
    print("\nè¾“å‡ºï¼š")
    print(f" - æ ‡å‡†åŒ–ç¼“å­˜: {CONFIG.get('NORM_CACHE_DIR','./eval_out/_norm_cache')}")
    print(" - æ˜ç»†:", os.path.relpath(detail_path))
    print(" - æ±‡æ€»:", os.path.relpath(summary_path))

    return detail_path, summary_path, report


# =============================
# [ADD] Gradio UIï¼ˆå¯¹é½ gradio_asr é£æ ¼ï¼‰
# =============================

def run_ui():
    # [ADD] UI æ‰©å±•ï¼šå…³é”®è¯ / é•¿å¥é˜ˆå€¼ / æ ‡ç‚¹é›† / ç›®æ ‡è¯­é€Ÿ
    try:
        import gradio as gr
    except Exception:
        print("[ERROR] æœªå®‰è£… gradioï¼›è¯·å…ˆ `pip install gradio`ï¼Œæˆ–ä½¿ç”¨ --ui 0 èµ° CLI")
        return

    CONTROL_URL = CONFIG["CONTROL_URL"]

    def _go_eval(csv_a_path, csv_b_path, out_dir, keywords, long_thres, punc_set, target_cps):
        # åŒæ—¶æ”¯æŒåŒCSVå®Œæ•´æŒ‡æ ‡
        args = _Args(csv_a_path.strip() or "", csv_b_path.strip() or "", out_dir.strip() or CONFIG["OUT_DIR_DEFAULT"])
        try:
            if (csv_a_path.strip() and csv_b_path.strip()):
                stdA = standardize_csv(csv_a_path.strip())
                stdB = standardize_csv(csv_b_path.strip())
                stdA = normalize_dataframe(stdA, 'A')
                stdB = normalize_dataframe(stdB, 'B')
                kws = [k.strip() for k in (keywords or '').split(',') if k.strip()] or CONFIG['KEYWORDS_DEFAULT']
                res = compare_two_full(stdA, stdB, int(long_thres), (punc_set or CONFIG['PUNC_FOR_PROSODY']), kws, (float(target_cps) if target_cps else CONFIG['TARGET_CHARS_PER_SEC']))
                d, s = _write_outputs(res, args.out_dir)
                rep = _summary_text(res)
                return rep, d, s
            else:
                d, s, rep = run_cli(args)
                return rep, d, s
        except Exception as e:
            return f"[ERROR] {e}", None, None

    with gr.Blocks(title="CSV å¯¹æ¯” / è¯„æµ‹ï¼ˆæœ€å°ç‰ˆï¼‰") as demo:
        gr.Markdown("# ğŸ“Š CSV å¯¹æ¯” / è¯„æµ‹ï¼ˆæœ€å°ç‰ˆï¼‰\nä¸Šä¼ è·¯å¾„ä¸æ˜¯å¿…é¡»ï¼Œ**ç›´æ¥å¡«è·¯å¾„æ›´é«˜æ•ˆ**ã€‚æ”¯æŒï¼šåŒCSVã€å•CSV(åŒASRåˆ—)ã€å•CSV(ASR vs REF)ã€‚")

        # ===== é¡¶éƒ¨å¯¼èˆªï¼ˆä¸ gradio_asr ä¸€è‡´ï¼‰ =====
        with gr.Row():
            btn_go_tts = gr.Button("TTS", variant="secondary")
            btn_go_eval = gr.Button("ç»“æœè¯„æµ‹", variant="secondary")
            btn_go_tune = gr.Button("å¾®è°ƒ", variant="secondary")
            btn_go_clean = gr.Button("æ•°æ®æ¸…æ´—", variant="secondary")
            btn_go_bridge = gr.Button("è¯­éŸ³æ¡¥æ¥", variant="secondary")
            btn_go_home = gr.Button("ä¸»æ§å°", variant="secondary")
        _js_to_tts = f"(x)=>{{ window.top.location.href = '{CONTROL_URL}?switch=TTS'; return []; }}"
        _js_to_eval = f"(x)=>{{ window.top.location.href = '{CONTROL_URL}?switch=EVAL'; return []; }}"
        _js_to_tune = f"(x)=>{{ window.top.location.href = '{CONTROL_URL}?switch=FINETUNE'; return []; }}"
        _js_to_clean = f"(x)=>{{ window.top.location.href = '{CONTROL_URL}?switch=CLEAN'; return []; }}"
        _js_to_bridge = f"(x)=>{{ window.top.location.href = '{CONTROL_URL}?switch=BRIDGE'; return []; }}"
        _js_to_home = f"(x)=>{{ window.top.location.href = '{CONTROL_URL}'; return []; }}"
        try:  # Gradio v4+
            btn_go_tts.click(None, [], [], js=_js_to_tts)
            btn_go_eval.click(None, [], [], js=_js_to_eval)
            btn_go_tune.click(None, [], [], js=_js_to_tune)
            btn_go_clean.click(None, [], [], js=_js_to_clean)
            btn_go_bridge.click(None, [], [], js=_js_to_bridge)
            btn_go_home.click(None, [], [], js=_js_to_home)
        except TypeError:  # å…¼å®¹ v3.x
            btn_go_tts.click(None, [], [], _js=_js_to_tts)
            btn_go_eval.click(None, [], [], _js=_js_to_eval)
            btn_go_tune.click(None, [], [], _js=_js_to_tune)
            btn_go_clean.click(None, [], [], _js=_js_to_clean)
            btn_go_bridge.click(None, [], [], _js=_js_to_bridge)
            btn_go_home.click(None, [], [], _js=_js_to_home)

        # ===== 3 ä¸ªè¾“å…¥æ¡† + ä¸€é”®å¯¹æ¯” =====
        with gr.Row():
            csv_a_path = gr.Textbox(label="CSV A è·¯å¾„ï¼ˆå•CSVæ—¶ä»…å¡« Aï¼‰", placeholder=r"D:\path\to\A.csv")
            csv_b_path = gr.Textbox(label="CSV B è·¯å¾„ï¼ˆå¯é€‰ï¼‰", placeholder=r"D:\path\to\B.csv")
            out_dir = gr.Textbox(label="è¾“å‡ºç›®å½•ï¼ˆç›¸å¯¹è·¯å¾„ä¼˜å…ˆï¼‰", value=CONFIG["OUT_DIR_DEFAULT"])
        btn_run = gr.Button("å¼€å§‹å¯¹æ¯”", variant="primary")
        gr.Markdown("**è¯´æ˜**ï¼šåŒCSVæ¨¡å¼å°†åŸºäº `A/B.ASR` å¯¹ `REF` è¿›è¡Œå…¨æŒ‡æ ‡è¯„æµ‹ï¼›è‹¥è¯»å–ä¸åˆ° WAV æ—¶é•¿ï¼Œåˆ™è¯­é€Ÿè¯¯å·®è‡ªåŠ¨è·³è¿‡ã€‚")

        # [ADD] ç¼ºå¤±çš„å››ä¸ªè¾“å…¥ç»„ä»¶ï¼ˆç”¨äº _go_eval çš„å‚æ•°ï¼‰
        with gr.Row():
            keywords = gr.Textbox(label="å…³é”®è¯åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰", value=",".join(CONFIG["KEYWORDS_DEFAULT"]))
            long_thres = gr.Number(label="é•¿å¥é˜ˆå€¼ï¼ˆå­—æ•°ï¼‰", value=CONFIG["LONG_SENT_THRES"])
            punc_set = gr.Textbox(label="æ ‡ç‚¹é›†åˆï¼ˆéŸµå¾‹ä»£ç†ï¼‰", value=CONFIG["PUNC_FOR_PROSODY"])
            target_cps = gr.Number(label="ç›®æ ‡è¯­é€Ÿï¼ˆå­—ç¬¦/ç§’ï¼‰", value=CONFIG["TARGET_CHARS_PER_SEC"])

        # ===== è¾“å‡ºåŒºï¼šæŠ¥å‘Šæ–‡æœ¬ + ä¸‹è½½æ–‡ä»¶ =====
        report_out = gr.Textbox(label="å¯¹æ¯”æ±‡æ€»æŠ¥å‘Š", lines=8)
        out_detail = gr.File(label="ä¸‹è½½ detail.csv")
        out_summary = gr.File(label="ä¸‹è½½ summary.csv")

        btn_run.click(_go_eval, [csv_a_path, csv_b_path, out_dir, keywords, long_thres, punc_set, target_cps], [report_out, out_detail, out_summary])

    # å¯åŠ¨å‚æ•°ï¼ˆä¸ asr å¯¹é½ï¼‰
    if CONFIG["GRADIO_QUEUE"]:
        demo.queue(api_open=False, max_size=32)
    demo.launch(
        server_name=CONFIG["GRADIO_SERVER_NAME"],
        server_port=CONFIG["DEFAULT_PORT"],
        share=False,
        inbrowser=False,
    )


# =============================
# [ADD] å…¥å£å‚æ•°
# =============================
if __name__ == '__main__':
    p = argparse.ArgumentParser()
    p.add_argument('--csv_a', type=str, default='', help='CSV A æ–‡ä»¶è·¯å¾„ï¼ˆå• CSV æ¨¡å¼ä»…éœ€ Aï¼‰')
    p.add_argument('--csv_b', type=str, default='', help='CSV B æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    p.add_argument('--out_dir', type=str, default=CONFIG["OUT_DIR_DEFAULT"], help='è¾“å‡ºç›®å½•ï¼ˆç›¸å¯¹è·¯å¾„ä¼˜å…ˆï¼‰')
    p.add_argument('--ui', type=int, default=1, help='1=æ‰“å¼€UIï¼Œ0=å‘½ä»¤è¡Œ')
    args = p.parse_args()

    if int(args.ui) == 1:
        run_ui()
    else:
        run_cli(args)
