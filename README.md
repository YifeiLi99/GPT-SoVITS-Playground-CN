# GPT-SoVITS-Playground-CN

> 基于 [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS) 的实验性改造版本，主要用于 **个人研究与实验**。  
> 在原始框架基础上，改造统一工程壳层（相对路径 `config.py`）、Gradio 多页面（TTS/ASR/微调/评测），并提供 **TTS → ASR → 评测** 的闭环工具链。  
> ⚠️ 不包含任何预训练权重与语音数据；本仓库默认禁止提交音频/权重（见 `.gitignore`）。

---

## ✨ 亮点特性

- **微调支持**：一键实验流程（支持轻量快速试验）；
- **评测闭环**：TTS 生成 → ASR 转写 → 指标计算（WER/CER/口型一致性占位）→ 报告输出；
- **交互式工具**：Gradio 多页面（TTS / ASR / Finetune / Eval），一键对比；

---

## 🧪 代表性实验（v2ProPlus，中文，26h 原始语音）

> 目的：验证在 **v2ProPlus** 版本下，中文语音数据进行 **约 26 小时原始 → 清洗后 ~22 小时** 的微调效果，对比基模与微调模型在 **可懂度/稳定性/相似度** 等方面的变化，并记录 RTF（实时性）表现与常见问题。

### 实验配置（可复现实验脚本）
- **Backbone**：GPT-SoVITS **v2ProPlus**（按官方发布版本，推理侧含 SV 类型声码器）
- **数据**：中文普通话有声片段，**原始 ~26h → 清洗后 ~22h**  
  - 清洗规则（示例）：
    - 丢弃极短片段：`duration < 1.2s`；
    - 剔除异常能量/F0 片段、过噪样本；
    - 统一标注 CSV，拆分 `train/val/test`；
  - 参考子集：从干净片段中人工挑选 **2–5s** 参考音频若干（推理时控制音色/风格）。
- **环境建议**：
  - Python 3.10 / PyTorch（按 CUDA 版本单独安装）
  - 显存建议 ≥8GB；首选 Linux / Windows 环境
- **训练参数**：
  - batch size：依显存与实现 `2-8`；
  - 训练轮次：10左右即可
  - （可选）LoRA/全量微调二选一（按基模适配性选择）

> 注：上面参数为**模板**，请根据你的机器/数据自由调整；本仓库提供了一个“快速试验”的脚手架以减少改动成本。

### 推理与 RTF 记录
- **RTF 定义**：`生成耗时 / 音频时长`（项目内已在推理链路打点，可在日志中输出）。
- **建议记录**：短句/长句各 3–5 条，统计均值±标准差，方便和基模对比。

### 评测与指标
- **ASR 指标**：
  - **WER/CER**：使用同一 ASR（如 Whisper 系列）对基模与微调产出分别转写，与参考文本对齐后计算；
  - **容错**：统一文本清洗（大小写/标点/全半角）后再计算，避免前端差异带来的误差；
- **主观感知**：
  - **相似度**（音色贴合度）、**自然度**（无失真/金属感/“磨砂感”）、**流畅度**（连贯/停顿）
  - 可采用 1–5 分打分或 A/B 盲测问卷；
- **可选客观代理**：
  - **F0 轮廓（均值/方差）**、**能量分布** 对齐度；


**观察与备注（示例）**：  
- 微调用途更贴合目标音色，相似度↑；长句停顿更稳定；
- 个别样本在高频段出现“**磨砂感**”/边界噪，疑似与声码器带宽与训练数据高频特性相关；
- 参考音频质量对推理结果影响显著（2–5s 的选择对情感/音色有拉动）；
- 语气词、语调变化与训练集分布强相关；若训练集中大量保留口语语气词，推理可能更频繁地“学到”这些习惯。

### 已知问题与改进方向
- **“磨砂感”**：可尝试提升参考样本 SNR、在数据侧做高频噪声控制；必要时调整声码器或尝试更干净的目标域数据；
- **标点/停顿敏感性**：清洗与前端规则需要一致；建议在评测集里构造不同标点版本；
- **短句泛化**：过多极短片段训练可能导致语调碎片化，建议和中长句比例平衡；

---
## 🎧 模型合成音频示例

下面展示 6 段合成音频，每段下面附上对应的文本。

### 1️⃣ 日常对话
<audio controls>
  <source src="samples/baseline001.wav" type="audio/wav">
</audio>

**Baseline**：你说我昨晚熬夜啦?哎呀...被你发现啦。其实是因为我在追剧，因为太好看了，停不下来了呢。

<audio controls>
  <source src="samples/paimeng001.wav" type="audio/wav">
</audio>

**Finetuned**：你说我昨晚熬夜啦?哎呀...被你发现啦。其实是因为我在追剧，因为太好看了，停不下来了呢。

### 2️⃣ 正式对话
<audio controls>
  <source src="samples/baseline002.wav" type="audio/wav">
</audio>  

**Baseline**：如果我们把语音合成看作是一场音乐表演，那么每一个音素就是乐器的音符，只有在合适的时机停顿，才能让整体听起来更加自然。

<audio controls>
  <source src="samples/paimeng002.wav" type="audio/wav">
</audio>  

**Finetuned**：如果我们把语音合成看作是一场音乐表演，那么每一个音素就是乐器的音符，只有在合适的时机停顿，才能让整体听起来更加自然。

### 3️⃣ 多语气与情绪
<audio controls>
  <source src="samples/baseline003.wav" type="audio/wav">
</audio>  

**Baseline**：嗯……好像是这样吧？欸，不过我还是有点怀疑啊。哎呀，算啦算啦，我们继续。

<audio controls>
  <source src="samples/paimeng003.wav" type="audio/wav">
</audio>  

**Finetuned**：嗯……好像是这样吧？欸，不过我还是有点怀疑啊。哎呀，算啦算啦，我们继续。

---

## 🛠️ 快速开始

1) 克隆并创建环境
```bash
git clone https://github.com/<yourname>/GPT-SoVITS-Playground-CN.git
cd GPT-SoVITS-Playground-CN

# 注意：后续创建环境部分最好参照GPT-SoVITS原作者的攻略创建
# 本项目仅测试过Python3.11+PyTorch2.5.1+CUDA12.4环境是稳定的
```

2) **按需**编辑 `config.py`（**原作者**）与`my_config.py`（**新增**）

3) 准备你的 `datasets/`

4) 启动 Gradio 主页面：
```bash
python main_launcher.py
```
- **TTS**：文本转语音  
- **ASR**：语音转文本  
- **Finetune**：快速微调（实验向）  
- **Eval**：TTS→ASR 指标计算与报告导出
- **后续小功能**：（暂未写完）

---

## 🧭 评测说明（闭环）

1. **数据准备**：将待测文本/音频整理为 CSV；  
2. **TTS 生成**：用基模 / 微调模型分别合成；  
3. **ASR 转写**：统一同一个 ASR 模型进行转写；  
4. **指标计算**：计算 CER/WER/RTF 并自动汇总为报告（`Eval` 页面/脚本）；  
5. **主观评测**：可选问卷（MOS/相似度），JSON/CSV 归档；

> 目标是“快速对比 → 快速迭代”，减少仅凭耳朵判断带来的偏差。

---

## 📜 License & Acknowledgements

- 基于并致谢：**GPT-SoVITS**（见上游仓库及其 LICENSE 条款）  
- 本仓库 **仅用于个人研究与实验**；**不包含** 任何预训练权重与语音数据。

